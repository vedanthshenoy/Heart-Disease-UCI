{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be continuing the same procedures until we used RFE in the last section. Instead of that here we will be using SelectFroModel and for keeping it simple we will use only linear models.\n",
    "\n",
    "Applying SelectFromModel method working on linear model is after using L1 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier   #Multi-Layer Perceptron Classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "dataset=pd.read_csv('heart-disease-data.csv')\n",
    "\n",
    "print(dataset.head(5))     #prints first 5 values for all columns\n",
    "\n",
    "array=dataset.values\n",
    "data=array[:,0:13]\n",
    "labels=array[:,13]\n",
    "#model1=MLPClassifier(activation='relu',solver='lbfgs',alpha=1e-5,random_state=1,) #doesnot work with neural networks\n",
    "model2=LinearSVC(kernel='linear')    \n",
    "model3=LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data and Fitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Select from model case, we need to first fit the model to some algorithm (preferably SVC or Logistic Regression).\n",
    "\n",
    "Then select the important features which gives best accuracy when fed to that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVC model on unseen data is 78.68852459016394 %\n",
      "Accuracy of the Logistic regression model on unseen data is 81.9672131147541 %\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.2,random_state=4)\n",
    "\n",
    "\n",
    "model2=LinearSVC(C=0.01, penalty=\"l1\", dual=False)\n",
    "model2.fit(x_train,y_train)\n",
    "k2=model2.predict(x_test)\n",
    "print(\"Accuracy of the SVC model on unseen data is \"+str(accuracy_score(k2,y_test)*100)+\" %\")\n",
    "\n",
    "model3=LogisticRegression(C=0.01)\n",
    "model3.fit(x_train,y_train)\n",
    "k3=model3.predict(x_test)\n",
    "print(\"Accuracy of the Logistic regression model on unseen data is \"+str(accuracy_score(k3,y_test)*100)+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the sklearn in-built model SelectFromModel.\n",
    "\n",
    "We try this on the models SVC and Logistic regression.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "mainModel1=SelectFromModel(model2, prefit=True)\n",
    "mainModel2=SelectFromModel(model3, prefit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape is :  (303, 13)\n",
      "New data shape after applying SelectFromModel on SVC :  (303, 7)\n",
      "New data shape after applying SelectFromModel on Logistic Regression :  (303, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data shape is : \",data.shape)\n",
    "\n",
    "data_new1=mainModel1.transform(data)\n",
    "print(\"New data shape after applying SelectFromModel on SVC : \",data_new1.shape)\n",
    "\n",
    "data_new2=mainModel2.transform(data)\n",
    "print(\"New data shape after applying SelectFromModel on Logistic Regression : \",data_new2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the column value which represents the features has reduced to selecting the best of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting LinearSVC on the Transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on unseen transformed data : 90.1639344262295 %\n"
     ]
    }
   ],
   "source": [
    "data1=data_new1[:,0:6]\n",
    "labels1=data_new1[:,6]\n",
    "\n",
    "x_train1,x_test1,y_train1,y_test1=train_test_split(data,labels,test_size=0.2,random_state=4)\n",
    "\n",
    "model=LinearSVC()\n",
    "model.fit(x_train1,y_train1)\n",
    "\n",
    "k=model.predict(x_test)\n",
    "\n",
    "print(\"Accuracy of the model on unseen transformed data : \"+str(accuracy_score(k,y_test)*100)+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the transformed model has better accuracy than the model trained on normal data trained on LinearSVC which gave 78.68852459016394 % accuracy....Well here i'm just lucky ;p..Normally it is not the case, with feature reduction you guarentee the model won't be subjected to overfitting...This i'll just keep aside to preserve my luck...Please understand.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on unseen transformed data : 91.80327868852459 %\n"
     ]
    }
   ],
   "source": [
    "data2=data_new2[:,0:4]\n",
    "labels2=data_new1[:,4]\n",
    "\n",
    "x_train2,x_test2,y_train2,y_test2=train_test_split(data,labels,test_size=0.2,random_state=4)\n",
    "\n",
    "model=LinearSVC()\n",
    "model.fit(x_train1,y_train1)\n",
    "\n",
    "k=model.predict(x_test)\n",
    "\n",
    "print(\"Accuracy of the model on unseen transformed data : \"+str(accuracy_score(k,y_test)*100)+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too we see a rise in accuracy from 81.9672131147541 % to 91.80327868852459 %..Again here i tried a lot to get the accuracy score above the accuracy score without using feature reduction..This is not always the case,Sometimes you may never reach a better accuracy but one thing for sure is that feature reduction saves the model from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
